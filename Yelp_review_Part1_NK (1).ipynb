{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1jM-aceATAz"
   },
   "source": [
    "## Summary of the actions in this file: \"Yelp_review_Part1_NK.ipynb\"  \n",
    "\n",
    "#### PLEASE UPDATE THE read_path / write_path IN FIRST CODE CELL IF YOU NEED TO!\n",
    "\n",
    "The below code takes the data, which has circa 5.5 million rows, and using pyspark df, it does, in order, the below actions:  \n",
    "1) creates an additional column where ratings are given under \"label\" as float.  \n",
    "2) creates another df that only includes user_id, text and label columns  \n",
    "3) processes (cleans and lemmatizes) all text columns and uses these as \"words\" column  \n",
    "4) eliminates all rows that do not have user_id information  \n",
    "5) eliminates all rows that are not a 1 or 5 rating  \n",
    "6) counts the number of users (~1.1 million) that remain in the df; and the total number of (rating 1 and 5) reviews they have  \n",
    "7) filters the df to leave it only with teh information of teh users that have more than 20 reviews, so we are left with 12,356 users in df_shorter  \n",
    "8) saves df_shorter as yelp-cleaned  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 915,
     "status": "ok",
     "timestamp": 1711326857558,
     "user": {
      "displayName": "Nil Kalagoglu",
      "userId": "14388709165380564493"
     },
     "user_tz": 240
    },
    "id": "QZQwvBPnAWLd",
    "outputId": "1c05a9c4-0512-4c73-ddd8-b006cacf72ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# MOUNT GOOGLE DRIVE SO CAN READ THE FILE FROM THERE\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13917,
     "status": "ok",
     "timestamp": 1711326871473,
     "user": {
      "displayName": "Nil Kalagoglu",
      "userId": "14388709165380564493"
     },
     "user_tz": 240
    },
    "id": "wRxN-5GfAWyJ",
    "outputId": "c99dcd4a-3081-4ba6-ecd0-63c08452213a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1711326871474,
     "user": {
      "displayName": "Nil Kalagoglu",
      "userId": "14388709165380564493"
     },
     "user_tz": 240
    },
    "id": "kvZgJxveoQni"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import avg, sum, col\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "read_path = \"/content/drive/My Drive/Colab Notebooks/yelp-dataset/yelp_review.csv\"\n",
    "write_path = \"/content/drive/MyDrive/Colab Notebooks/yelp-cleaned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "executionInfo": {
     "elapsed": 146,
     "status": "error",
     "timestamp": 1711326919390,
     "user": {
      "displayName": "Nil Kalagoglu",
      "userId": "14388709165380564493"
     },
     "user_tz": 240
    },
    "id": "sjYfsQxuATA1",
    "outputId": "d3894a3a-cc3c-4ec9-b97b-fe0e176ad42b"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=yelpPart1, master=local[*]) created by __init__ at <ipython-input-9-46f90a8c86e3>:3 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-46f90a8c86e3>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local[*]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yelpPart1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    199\u001b[0m             )\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             self._do_init(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    450\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=yelpPart1, master=local[*]) created by __init__ at <ipython-input-9-46f90a8c86e3>:3 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"yelpPart1\")\n",
    "sc = SparkContext(conf = conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 370,
     "status": "ok",
     "timestamp": 1711326919886,
     "user": {
      "displayName": "Nil Kalagoglu",
      "userId": "14388709165380564493"
     },
     "user_tz": 240
    },
    "id": "xz8F8vZEqeQK"
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"multiline\",\"true\").load(read_path) #Reading the loaded csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 142,
     "status": "ok",
     "timestamp": 1711326920022,
     "user": {
      "displayName": "Nil Kalagoglu",
      "userId": "14388709165380564493"
     },
     "user_tz": 240
    },
    "id": "q_qRH75yztda",
    "outputId": "df6c75e6-88c2-4a8b-99ab-df94bf9f96d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- stars: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- useful: string (nullable = true)\n",
      " |-- funny: string (nullable = true)\n",
      " |-- cool: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5096,
     "status": "ok",
     "timestamp": 1711326925117,
     "user": {
      "displayName": "Nil Kalagoglu",
      "userId": "14388709165380564493"
     },
     "user_tz": 240
    },
    "id": "ySiKsmk53qA6",
    "outputId": "dd9effe8-00b7-4c60-adbd-b074d415eb4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----+----------+--------------------+------+-----+----+\n",
      "|           review_id|             user_id|         business_id|stars|      date|                text|useful|funny|cool|\n",
      "+--------------------+--------------------+--------------------+-----+----------+--------------------+------+-----+----+\n",
      "|vkVSCC7xljjrAI4UG...|bv2nCi5Qv5vroFiqK...|AEx2SYEUJmTxVVB18...|    5|2016-05-28|Super simple plac...|     0|    0|   0|\n",
      "|n6QzIUObkYshz4dz2...|bv2nCi5Qv5vroFiqK...|VR6GpWIda3SfvPC-l...|    5|2016-05-28|Small unassuming ...|     0|    0|   0|\n",
      "|MV3CcKScW05u5LVfF...|bv2nCi5Qv5vroFiqK...|CKC0-MOWMqoeWf6s-...|    5|2016-05-28|Lester's is locat...|     0|    0|   0|\n",
      "|IXvOzsEMYtiJI0CAR...|bv2nCi5Qv5vroFiqK...|ACFtxLv8pGrrxMm6E...|    4|2016-05-28|Love coming here....|     0|    0|   0|\n",
      "|L_9BTb55X0GDtThi6...|bv2nCi5Qv5vroFiqK...|s2I_Ni76bjJNK9yG6...|    4|2016-05-28|Had their chocola...|     0|    0|   0|\n",
      "|HRPm3vEZ_F-33TYVT...|_4iMDXbXZ1p1ONG29...|8QWPlVQ6D-OExqXoa...|    5|2014-09-24|Cycle Pub Las Veg...|     1|    0|   0|\n",
      "|ymAUG8DZfQcFTBSOi...|u0LXt3Uea_GidxRW1...|9_CGhHMz8698M9-Pk...|    4|2012-05-11|Who would have gu...|     0|    0|   2|\n",
      "|8UIishPUD92hXtScS...|u0LXt3Uea_GidxRW1...|gkCorLgPyQLsptTHa...|    4|2015-10-27|Always drove past...|     1|    0|   0|\n",
      "|w41ZS9shepfO3uEyh...|u0LXt3Uea_GidxRW1...|5r6-G9C4YLbC7Ziz5...|    3|2013-02-09|Not bad!! Love th...|     1|    0|   0|\n",
      "|WF_QTN3p-thD74hqp...|u0LXt3Uea_GidxRW1...|fDF_o2JPU8BR1Gya-...|    5|2016-04-06|Love this place!\\...|     3|    0|   0|\n",
      "|PIsUSmvaUWB00qv5K...|u0LXt3Uea_GidxRW1...|z8oIoCT1cXz7gZP5G...|    4|2013-05-01|This is currently...|     1|    0|   0|\n",
      "|PdZ_uFjbbkjtm3SCY...|u0LXt3Uea_GidxRW1...|XWTPNfskXoUL-Lf32...|    3|2011-09-28|Server was a litt...|     5|    0|   1|\n",
      "|x5oV6wm9_Pb1QQ6jk...|u0LXt3Uea_GidxRW1...|13nKUHH-uEUXVZylg...|    1|2011-02-16|I thought Tidy's ...|     9|    0|   1|\n",
      "|lsoSqIrrDbQvWpMvs...|u0LXt3Uea_GidxRW1...|RtUvSWO_UZ8V3Wpj0...|    3|2012-12-03|Wanted to check o...|     2|    1|   1|\n",
      "|23eqwlZzCWZkADWfd...|u0LXt3Uea_GidxRW1...|Aov96CM4FZAXeZvKt...|    5|2010-07-16|This place is awe...|     2|    0|   1|\n",
      "|FunI9om-aK5oMIIJm...|u0LXt3Uea_GidxRW1...|0W4lkclzZThpx3V65...|    4|2011-09-28|a must stop when ...|     0|    0|   0|\n",
      "|FKu4iU62EmWT6GZXP...|u0LXt3Uea_GidxRW1...|fdnNZMk1NP7ZhL-YM...|    1|2012-10-23|I too have been t...|     0|    0|   0|\n",
      "|xdu8nXrbNKeaywCX7...|u0LXt3Uea_GidxRW1...|PFPUMF38-lraKzLcT...|    3|2010-09-15|Came here with my...|     2|    0|   0|\n",
      "|K7o5jDInfmX3cY5oH...|u0LXt3Uea_GidxRW1...|oWTn2IzrprsRkPfUL...|    3|2012-09-23|Came here for a b...|     4|    0|   0|\n",
      "|WYDFJOBOl7cycd7gN...|u0LXt3Uea_GidxRW1...|zgQHtqX0gqMw1nlBZ...|    1|2012-10-30|really excited to...|     9|    2|   1|\n",
      "+--------------------+--------------------+--------------------+-----+----------+--------------------+------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 154,
     "status": "ok",
     "timestamp": 1711326925268,
     "user": {
      "displayName": "Nil Kalagoglu",
      "userId": "14388709165380564493"
     },
     "user_tz": 240
    },
    "id": "7Lj0q3Pv3soq"
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('label', df[\"stars\"].cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 210,
     "status": "ok",
     "timestamp": 1711326975062,
     "user": {
      "displayName": "Nil Kalagoglu",
      "userId": "14388709165380564493"
     },
     "user_tz": 240
    },
    "id": "yf-WXmlP30jD",
    "outputId": "2c9cf1e3-1f5f-4796-c496-1ea3f988d1ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- stars: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- useful: string (nullable = true)\n",
      " |-- funny: string (nullable = true)\n",
      " |-- cool: string (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 293,
     "status": "ok",
     "timestamp": 1711326975349,
     "user": {
      "displayName": "Nil Kalagoglu",
      "userId": "14388709165380564493"
     },
     "user_tz": 240
    },
    "id": "tYtQdFXxATA3",
    "outputId": "e2ece4df-b629-47bb-88f7-f4cfac63fe0c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords') # if needed\n",
    "nltk.download('punkt') # if needed\n",
    "nltk.download('wordnet') # if needed\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pyspark.sql.functions import udf # udf takes a function and returns a function that can be applied to a column\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "import string\n",
    "\n",
    "stopword_list = set(stopwords.words(\"english\"))\n",
    "\n",
    "punkt_list = set(string.punctuation)\n",
    "\n",
    "def lemmatized(word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(word)\n",
    "\n",
    "def ProcessText(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    processed_tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stopword_list]\n",
    "    processed_tokens = [lemmatized(word) for word in processed_tokens]\n",
    "    return processed_tokens\n",
    "\n",
    "# a function that takes a column of text and returns a column of processed text\n",
    "process_text_udf = udf(ProcessText, StringType()) # the StringType() is the return type of the function\n",
    "\n",
    "# we use the udf to create a user defined function \"ProcessText\" that takes a column of text and returns a column of processed text\n",
    "# udf ensures that each cell in our column is processed by the function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 161,
     "status": "ok",
     "timestamp": 1711326975504,
     "user": {
      "displayName": "Nil Kalagoglu",
      "userId": "14388709165380564493"
     },
     "user_tz": 240
    },
    "id": "5aAqVmuIATA3"
   },
   "outputs": [],
   "source": [
    "df_2 = df['user_id','text','label']\n",
    "\n",
    "# we apply a mapping function ProcessText to all values in teh text column -\n",
    "# the result is stored in a new column called words\n",
    "# we need to pass each row in teh column to the function by one by - teh function expects a string not a column\n",
    "\n",
    "df_3= df_2.withColumn(\"words\", process_text_udf(col(\"text\")))\n",
    "\n",
    "df_short = df_3['user_id','words','label']\n",
    "\n",
    "# now we will groupby user_id first and label second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3101,
     "status": "ok",
     "timestamp": 1711326978604,
     "user": {
      "displayName": "Nil Kalagoglu",
      "userId": "14388709165380564493"
     },
     "user_tz": 240
    },
    "id": "SEAk-P6k38aT",
    "outputId": "07cde35b-3b34-4d66-c5b5-bff0a65913bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|             user_id|               words|label|\n",
      "+--------------------+--------------------+-----+\n",
      "|bv2nCi5Qv5vroFiqK...|[super, simple, p...|  5.0|\n",
      "|bv2nCi5Qv5vroFiqK...|[small, unassumin...|  5.0|\n",
      "|bv2nCi5Qv5vroFiqK...|[lester, located,...|  5.0|\n",
      "|bv2nCi5Qv5vroFiqK...|[love, coming, ye...|  4.0|\n",
      "|bv2nCi5Qv5vroFiqK...|[chocolate, almon...|  4.0|\n",
      "|_4iMDXbXZ1p1ONG29...|[cycle, pub, la, ...|  5.0|\n",
      "|u0LXt3Uea_GidxRW1...|[would, guess, wo...|  4.0|\n",
      "|u0LXt3Uea_GidxRW1...|[always, drove, p...|  4.0|\n",
      "|u0LXt3Uea_GidxRW1...|[bad, love, vegan...|  3.0|\n",
      "|u0LXt3Uea_GidxRW1...|[love, place, peg...|  5.0|\n",
      "|u0LXt3Uea_GidxRW1...|[currently, paren...|  4.0|\n",
      "|u0LXt3Uea_GidxRW1...|[server, little, ...|  3.0|\n",
      "|u0LXt3Uea_GidxRW1...|[thought, tidy, f...|  1.0|\n",
      "|u0LXt3Uea_GidxRW1...|[wanted, check, p...|  3.0|\n",
      "|u0LXt3Uea_GidxRW1...|[place, awesome, ...|  5.0|\n",
      "|u0LXt3Uea_GidxRW1...|[must, stop, mont...|  4.0|\n",
      "|u0LXt3Uea_GidxRW1...|[trying, book, ap...|  1.0|\n",
      "|u0LXt3Uea_GidxRW1...|[came, girlfriend...|  3.0|\n",
      "|u0LXt3Uea_GidxRW1...|[came, burger, on...|  3.0|\n",
      "|u0LXt3Uea_GidxRW1...|[really, excited,...|  1.0|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_short.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1711326978604,
     "user": {
      "displayName": "Nil Kalagoglu",
      "userId": "14388709165380564493"
     },
     "user_tz": 240
    },
    "id": "-1R3qXNSeBuh"
   },
   "outputs": [],
   "source": [
    "# df.count() # returns 5427013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 98868,
     "status": "ok",
     "timestamp": 1711327077469,
     "user": {
      "displayName": "Nil Kalagoglu",
      "userId": "14388709165380564493"
     },
     "user_tz": 240
    },
    "id": "ZSw3uIQXATA3",
    "outputId": "358f5b29-0950-4a2c-ae8a-0242355d7d40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|             user_id|count|\n",
      "+--------------------+-----+\n",
      "| on a beautiful P...|    1|\n",
      "|Q87V0vOAtbpxdkrF6...|    1|\n",
      "|1yGnVPN0ORgvLr_pM...|    1|\n",
      "|PqUQyGApS2pho7aox...|    1|\n",
      "|LqdHGAYxwICIXuqfU...|    1|\n",
      "|32vx6QPtlUvMFurvb...|    1|\n",
      "| given the swathe...|    1|\n",
      "| go to another lo...|    1|\n",
      "|wpHRE_R8rOWp1JQur...|    1|\n",
      "|bSA6m2r1k67uaEPCc...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We will count the number of rows with the same user_id information.\n",
    "counts_df = df_short.groupBy(\"user_id\").count()\n",
    "counts_df.sort(\"count\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 288,
     "status": "ok",
     "timestamp": 1711327077747,
     "user": {
      "displayName": "Nil Kalagoglu",
      "userId": "14388709165380564493"
     },
     "user_tz": 240
    },
    "id": "2vRGgrQmATA4"
   },
   "outputs": [],
   "source": [
    "# The data shows that there are various rows without a user ID. We will remove these rows from the dataset.\n",
    "# We also notice that subscribed User_id's include alpha characters, and the ones that reviewed without subscription have smaller pure number user_id's.\n",
    "# To filer for these, we will also exclude any user_id's that do not contain alphabet character.\n",
    "\n",
    "df_short = df_short.filter(df_short.user_id != '')\n",
    "df_short = df_short.filter(df_short.user_id.rlike('[a-zA-Z]'))  # rlike is a regex function that filters for rows that contain alphabet characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1711327077748,
     "user": {
      "displayName": "Nil Kalagoglu",
      "userId": "14388709165380564493"
     },
     "user_tz": 240
    },
    "id": "dH71nJsG39Tc"
   },
   "outputs": [],
   "source": [
    "# We also eliminate entries where there is no 1 or 5 rating associated\n",
    "\n",
    "df_short = df_short.filter(df.label.isin(1.0,5.0)) # We only want to keep the reviews that are rated as 1 or 5 to see what turns off a customer and what really pleases them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 81638,
     "status": "ok",
     "timestamp": 1711327159383,
     "user": {
      "displayName": "Nil Kalagoglu",
      "userId": "14388709165380564493"
     },
     "user_tz": 240
    },
    "id": "CBD4aqZx4A5H",
    "outputId": "287038a4-677c-4abb-ae67-22616edd3fb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|             user_id|count|\n",
      "+--------------------+-----+\n",
      "|dt9IHwfuZs9D9LOH7...|  387|\n",
      "|RBZ_kMjowV0t6_nv2...|  326|\n",
      "|rCWrxuRC8_pfagpch...|  307|\n",
      "|JLv2Dmfj73-I0d9N4...|  302|\n",
      "|ELcQDlf69kb-ihJfx...|  301|\n",
      "|U4INQZOPSUaj8hMjL...|  289|\n",
      "|7sNE58P4AvsX6QHE8...|  288|\n",
      "|dIIKEfOgo0KqUfGQv...|  276|\n",
      "|cMEtAiW60I5wE_vLf...|  272|\n",
      "|G-_KF_Ul4d3WGEa-G...|  268|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counts_df = df_short.groupBy(\"user_id\").count()\n",
    "counts_df.persist()\n",
    "counts_df.sort(\"count\", ascending = False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3357,
     "status": "ok",
     "timestamp": 1711327162729,
     "user": {
      "displayName": "Nil Kalagoglu",
      "userId": "14388709165380564493"
     },
     "user_tz": 240
    },
    "id": "in-1SczlATA4",
    "outputId": "20605d83-79c8-419f-d6f9-d103f13894f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: string, count: bigint]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_df.count() # returns 1,127,806 users\n",
    "counts_df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KhEu9-xATA4"
   },
   "source": [
    "# The below cell will collect - so it takes long time to run!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Mr4rAViATA4"
   },
   "source": [
    "There are 1,127,806 users in our df, with the majority of the users have only written few reviews\n",
    "We will create a list of user ID's from our df_short that have posted more than 20 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7TOgIcB5ATA4"
   },
   "outputs": [],
   "source": [
    "r = 20\n",
    "user_list = counts_df.filter(counts_df[\"count\"] > r).select(\"user_id\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# We will filter our df_short to only include users who have posted more than 20 reviews\n",
    "df_shorter = df_short.filter(df_short.user_id.isin(user_list))\n",
    "df_shorter.persist() # this will cache the dataframe in memory and it is useful because we use it to save it to a file and to count the rows later.\n",
    "\n",
    "counts_df = df_shorter.groupBy(\"user_id\").count()\n",
    "\n",
    "counts_df.count() # returns 12,356 users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oX2MMFarATA4"
   },
   "source": [
    "The below code could have been a good alternative to the above, since it does not collect.  \n",
    "Can work on it later, when things run faster on cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WleKlG9hATA4"
   },
   "outputs": [],
   "source": [
    "# # this one, had it worked, could have been more efficient than teh previous cell, as it does not collect, but so far it does not work.\n",
    "\n",
    "# # we join a filtered user_id's with the original dataframe to get the reviews of the users who have posted more than 20 reviews\n",
    "# # we do not want to have a count column in our final dataframe, so we will drop it\n",
    "\n",
    "# df_filtered = df_short.groupBy(\"user_id\").count().filter(\"count > 20\")\n",
    "# df_shorter = df_filtered.join(df_short, \"user_id\", \"inner\").drop(\"count\") # something is wrong with this line - maybe the user_id is ambiguous?\n",
    "# df_shorter.persist()\n",
    "\n",
    "# counts_df = df_shorter.groupBy(\"user_id\").count()\n",
    "# counts_df.count() # if it works properly, it should return 12356"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6p2o_K3ATA5"
   },
   "outputs": [],
   "source": [
    "df_shorter.count() # returns X rows - the length of our shorter df ; do not run - it will take long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3vB3Zq8oATA5"
   },
   "outputs": [],
   "source": [
    "df_shorter.write.csv(write_path) # so we can use this cleaned data in the future without having to re-run the whole code"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
